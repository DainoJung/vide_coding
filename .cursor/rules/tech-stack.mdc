---
description: tech stack
globs: 
alwaysApply: false
---
# ìµœì €ê°€ ê²€ìƒ‰ Agent ê¸°ìˆ  ìŠ¤íƒ

## ğŸ”§ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

### ì–¸ì–´ ë° ëŸ°íƒ€ì„
- **Python**: 3.11 (ìµœì‹  ì„±ëŠ¥ ìµœì í™” ë° íƒ€ì… íŒíŒ… ì§€ì›)
- **íŒ¨í‚¤ì§€ ê´€ë¦¬**: Poetry ë˜ëŠ” pip + requirements.txt

### Frontend
- **Streamlit**: ëŒ€í™”í˜• ì›¹ ì¸í„°í˜ì´ìŠ¤
  - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì§€ì›
  - ê°„ë‹¨í•œ ë°°í¬ ë° í”„ë¡œí† íƒ€ì´í•‘
  - ìƒíƒœ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€

### Backend API
- **FastAPI**: RESTful API ì„œë²„
  - ìë™ API ë¬¸ì„œ ìƒì„± (OpenAPI/Swagger)
  - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›ìœ¼ë¡œ ë³‘ë ¬ ê²€ìƒ‰ ìµœì í™”
  - Pydanticì„ í†µí•œ ë°ì´í„° ê²€ì¦

### AI Agent Framework
- **LangGraph**: ëŒ€í™”í˜• Agent êµ¬í˜„
  - ìƒíƒœ ê´€ë¦¬ ë° ì›Œí¬í”Œë¡œìš° ì œì–´
  - **ë¡œì»¬ ë©”ëª¨ë¦¬ ëª¨ë“ˆ** ì‚¬ìš©ìœ¼ë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸°ì–µ
  - ë…¸ë“œ ê¸°ë°˜ Agent êµ¬ì„±ìœ¼ë¡œ í™•ì¥ì„± í™•ë³´

### ëª¨ë‹ˆí„°ë§ ë° ì¶”ì 
- **LangSmith**: Agent ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
  - ëŒ€í™” í”Œë¡œìš° ì¶”ì 
  - ë””ë²„ê¹… ë° ì„±ëŠ¥ ë¶„ì„
  - Agent í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ

### LLM (Large Language Model)
- **Gemini 2.5 Flash Preview (05-20)**: ì£¼ ì–¸ì–´ ëª¨ë¸
  - ë¹ ë¥¸ ì‘ë‹µ ì†ë„ (Flash ìµœì í™”)
  - í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
  - ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì§€ì›

### Tools & Integrations
- **MCP (Model Context Protocol)**: ë„êµ¬ ì—°ë™ í”„ë¡œí† ì½œ
  - í‘œì¤€í™”ëœ ë„êµ¬ ì¸í„°í˜ì´ìŠ¤
  - í™•ì¥ ê°€ëŠ¥í•œ ë„êµ¬ ìƒíƒœê³„

### ê²€ìƒ‰ ë„êµ¬
- **DuckDuckGo Search MCP**: ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥
  - í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ ê²€ìƒ‰
  - ì‡¼í•‘ëª° ì •ë³´ ë³´ì¡° ê²€ìƒ‰
  - ìƒí’ˆ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì„±

```mermaid
graph TB
    subgraph "Frontend Layer"
        ST[Streamlit UI]
    end
    
    subgraph "API Layer"
        API[FastAPI Server]
        Auth[ì¸ì¦ ë¯¸ë“¤ì›¨ì–´]
    end
    
    subgraph "AI Agent Layer"
        LG[LangGraph Agent]
        Memory[ë¡œì»¬ ë©”ëª¨ë¦¬ ëª¨ë“ˆ]
        LLM[Gemini 2.5 Flash]
    end
    
    subgraph "Tools Layer"
        MCP[MCP Protocol]
        DDG[DuckDuckGo MCP]
        Shopping[ì‡¼í•‘ëª° í¬ë¡¤ëŸ¬]
    end
    
    subgraph "Monitoring"
        LS[LangSmith]
    end
    
    ST --> API
    API --> Auth
    Auth --> LG
    LG --> Memory
    LG --> LLM
    LG --> MCP
    MCP --> DDG
    MCP --> Shopping
    LG --> LS
```

## ğŸ“¦ ì£¼ìš” Python íŒ¨í‚¤ì§€

### Core Dependencies
```python
# requirements.txt ì˜ˆì‹œ
fastapi==0.104.1
streamlit==1.28.1
langgraph==0.0.40
langsmith==0.0.64
google-generativeai==0.3.1  # Gemini API
uvicorn==0.24.0  # ASGI ì„œë²„
pydantic==2.5.0
```

### ê²€ìƒ‰ ë° í¬ë¡¤ë§
```python
httpx==0.25.1  # ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸
beautifulsoup4==4.12.2  # HTML íŒŒì‹±
playwright==1.40.0  # ë¸Œë¼ìš°ì € ìë™í™”
```

### ë°ì´í„° ì²˜ë¦¬
```python
pandas==2.1.3  # ë°ì´í„° ì²˜ë¦¬
numpy==1.25.2  # ìˆ˜ì¹˜ ê³„ì‚°
```

### MCP Tools
```python
mcp==0.1.0  # MCP í´ë¼ì´ì–¸íŠ¸
duckduckgo-mcp==0.1.0  # DuckDuckGo ê²€ìƒ‰ ë„êµ¬
```

## ğŸ”„ êµ¬í˜„ ì•„í‚¤í…ì²˜

### 1. Streamlit Frontend
```python
# app.py
import streamlit as st
from typing import AsyncGenerator

async def stream_search_results(query: str) -> AsyncGenerator[str, None]:
    """ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
    async with httpx.AsyncClient() as client:
        async with client.stream('POST', '/api/search', json={'query': query}) as response:
            async for chunk in response.aiter_text():
                yield chunk

# ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ í‘œì‹œ
if user_input := st.chat_input("ìƒí’ˆëª… ë˜ëŠ” URLì„ ì…ë ¥í•˜ì„¸ìš”"):
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        async for chunk in stream_search_results(user_input):
            full_response += chunk
            response_placeholder.write(full_response)
```

### 2. FastAPI Backend
```python
# main.py
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langgraph import StateGraph

app = FastAPI()

@app.post("/api/search")
async def search_products(request: SearchRequest):
    """ìƒí’ˆ ê²€ìƒ‰ API with ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    
    async def generate_response():
        # LangGraph Agent ì‹¤í–‰
        agent = create_search_agent()
        
        async for step in agent.astream({"query": request.query}):
            yield f"data: {json.dumps(step)}\n\n"
    
    return StreamingResponse(generate_response(), media_type="text/plain")
```

### 3. LangGraph Agent
```python
# agent.py
from langgraph import StateGraph
from langgraph.memory import MemorySaver  # ë¡œì»¬ ë©”ëª¨ë¦¬ ëª¨ë“ˆ

class SearchState(TypedDict):
    query: str
    search_results: List[dict]
    conversation_history: List[dict]

def create_search_agent():
    # ë¡œì»¬ ë©”ëª¨ë¦¬ ì„¤ì •
    memory = MemorySaver()
    
    # Agent ì›Œí¬í”Œë¡œìš° ì •ì˜
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì •ì˜
    workflow.add_node("parse_query", parse_user_query)
    workflow.add_node("search_parallel", search_shopping_malls)
    workflow.add_node("calculate_prices", calculate_best_prices)
    workflow.add_node("format_response", format_final_response)
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_edge("parse_query", "search_parallel")
    workflow.add_edge("search_parallel", "calculate_prices")
    workflow.add_edge("calculate_prices", "format_response")
    
    # ë©”ëª¨ë¦¬ ì ìš©
    return workflow.compile(checkpointer=memory)
```

### 4. MCP Tools Integration
```python
# tools/mcp_client.py
from mcp import ClientSession
from mcp.client.stdio import StdioServerParameters

async def setup_mcp_tools():
    """MCP ë„êµ¬ë“¤ ì„¤ì •"""
    
    # DuckDuckGo ê²€ìƒ‰ ë„êµ¬
    ddg_params = StdioServerParameters(
        command="python",
        args=["-m", "duckduckgo_mcp"]
    )
    
    async with ClientSession(ddg_params) as session:
        # ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tools = await session.list_tools()
        return tools

# Agentì—ì„œ ë„êµ¬ ì‚¬ìš©
async def search_with_ddg(query: str):
    tools = await setup_mcp_tools()
    result = await tools["duckduckgo_search"].call({"query": query})
    return result
```

## ğŸš€ ë°°í¬ ë° ì‹¤í–‰

### ê°œë°œ í™˜ê²½ ì‹¤í–‰
```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 2. FastAPI ì„œë²„ ì‹¤í–‰
uvicorn main:app --reload --port 8000

# 3. Streamlit ì•± ì‹¤í–‰ (ìƒˆ í„°ë¯¸ë„)
streamlit run app.py --server.port 8501
```

### í”„ë¡œë•ì…˜ ë°°í¬
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# FastAPIì™€ Streamlit ë™ì‹œ ì‹¤í–‰
CMD ["sh", "-c", "uvicorn main:app --host 0.0.0.0 --port 8000 & streamlit run app.py --server.port 8501 --server.address 0.0.0.0"]
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### ë¹„ë™ê¸° ì²˜ë¦¬
- FastAPIì˜ async/awaitë¡œ ë³‘ë ¬ ì‡¼í•‘ëª° ê²€ìƒ‰
- LangGraphì˜ ë¹„ë™ê¸° ë…¸ë“œ ì‹¤í–‰
- Streamlitì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›

### ë©”ëª¨ë¦¬ ê´€ë¦¬
- LangGraph ë¡œì»¬ ë©”ëª¨ë¦¬ë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
- ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ê²©ë¦¬
- ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ (TTL ì„¤ì •)

### ìºì‹± ì „ëµ
```python
from functools import lru_cache
import asyncio

@lru_cache(maxsize=100)
async def cached_product_search(product_name: str, site: str):
    """ìƒí’ˆ ê²€ìƒ‰ ê²°ê³¼ ìºì‹± (5ë¶„)"""
    # ìºì‹œ ë§Œë£Œ ì‹œê°„ ì„¤ì •
    await asyncio.sleep(0)  # ë¹„ë™ê¸° ìºì‹± ë¡œì§
    return search_result
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
```bash
# .env
GEMINI_API_KEY=your_gemini_api_key
LANGSMITH_API_KEY=your_langsmith_key
LANGSMITH_PROJECT=price-search-agent

# ì‡¼í•‘ëª° API í‚¤ë“¤
COUPANG_API_KEY=your_coupang_key
NAVER_CLIENT_ID=your_naver_id
NAVER_CLIENT_SECRET=your_naver_secret
```

### ì„¤ì • ê´€ë¦¬
```python
# config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    gemini_api_key: str
    langsmith_api_key: str
    langsmith_project: str = "price-search-agent"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

## ì°¸ê³  íŒŒì¼
- ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜: [system-architecture.mdc](mdc:system-architecture.mdc)
- PRD: [prd.mdc](mdc:prd.mdc)
- ì™€ì´ì–´í”„ë ˆì„: [wireframe.md](mdc:docs/wireframe.md)
- ë¬¸ì œ ì •ì˜: [problem-definition.mdc](mdc:problem-definition.mdc)
